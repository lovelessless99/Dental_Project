{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.layers import * \n",
    "from keras import layers, initializers, optimizers\n",
    "from keras import models\n",
    "from datetime import datetime\n",
    "\n",
    "from Dental_Tool.Data_processing import *\n",
    "from Dental_Tool.Process_results import *\n",
    "from Dental_Tool.Dataloader import *\n",
    "from Dental_Tool.KFold_v2 import *\n",
    "\n",
    "import keras.backend as K \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>State</th>\n",
       "      <th>Class</th>\n",
       "      <th>source</th>\n",
       "      <th>tooth_num</th>\n",
       "      <th>angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dental_Data/PBL/10_20200901/15-53-18-500_00040...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dental_Data/PBL/10_20200901_Flip/15-53-18-500_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dental_Data/PBL/10_clahe_20200901/16-31-08-628...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dental_Data/PBL/10_clahe_20200901_Flip/16-31-0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dental_Data/PBL/10_20200901/15-53-18-515_00040...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381435</th>\n",
       "      <td>Dental_Data/PBL/10_clahe_20200901_Flip/17-03-2...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381436</th>\n",
       "      <td>Dental_Data/PBL/10_20200901/16-30-51-036_S5949...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381437</th>\n",
       "      <td>Dental_Data/PBL/10_20200901_Flip/16-30-51-036_...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381438</th>\n",
       "      <td>Dental_Data/PBL/10_clahe_20200901/17-03-20-150...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381439</th>\n",
       "      <td>Dental_Data/PBL/10_clahe_20200901_Flip/17-03-2...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381440 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Path  State  Class  \\\n",
       "0       Dental_Data/PBL/10_20200901/15-53-18-500_00040...      1      0   \n",
       "1       Dental_Data/PBL/10_20200901_Flip/15-53-18-500_...      1      0   \n",
       "2       Dental_Data/PBL/10_clahe_20200901/16-31-08-628...      1      0   \n",
       "3       Dental_Data/PBL/10_clahe_20200901_Flip/16-31-0...      1      0   \n",
       "4       Dental_Data/PBL/10_20200901/15-53-18-515_00040...      1      0   \n",
       "...                                                   ...    ...    ...   \n",
       "381435  Dental_Data/PBL/10_clahe_20200901_Flip/17-03-2...      2      1   \n",
       "381436  Dental_Data/PBL/10_20200901/16-30-51-036_S5949...      2      1   \n",
       "381437  Dental_Data/PBL/10_20200901_Flip/16-30-51-036_...      2      1   \n",
       "381438  Dental_Data/PBL/10_clahe_20200901/17-03-20-150...      2      1   \n",
       "381439  Dental_Data/PBL/10_clahe_20200901_Flip/17-03-2...      2      1   \n",
       "\n",
       "                            source  tooth_num  angle  \n",
       "0        NN_191024_151623_BE78A8_6          6    -10  \n",
       "1        NN_191024_151623_BE78A8_6          6    -10  \n",
       "2        NN_191024_151623_BE78A8_6          6    -10  \n",
       "3        NN_191024_151623_BE78A8_6          6    -10  \n",
       "4        NN_191024_151623_BE78A8_6          6     -9  \n",
       "...                            ...        ...    ...  \n",
       "381435  NN_180917_113933_C0A0B2_26         26      8  \n",
       "381436  NN_180917_113933_C0A0B2_26         26      9  \n",
       "381437  NN_180917_113933_C0A0B2_26         26      9  \n",
       "381438  NN_180917_113933_C0A0B2_26         26      9  \n",
       "381439  NN_180917_113933_C0A0B2_26         26      9  \n",
       "\n",
       "[381440 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = [ \n",
    "                \"Dental_Data/PBL/10_20200901\", \"Dental_Data/PBL/10_20200901_Flip\", \n",
    "                \"Dental_Data/PBL/10_clahe_20200901\", \"Dental_Data/PBL/10_clahe_20200901_Flip\"\n",
    "            ]\n",
    "\n",
    "\n",
    "directory = [ i + \"/mapping.json\" for i in directory]\n",
    "argscale_num = 80\n",
    "\n",
    "data = load_json(directory)\n",
    "dataset = json_2_dataframe_PBL(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# import tensorflow as tf\n",
    "# from keras import initializers, layers\n",
    "\n",
    "# class Length(layers.Layer):\n",
    "#     \"\"\"\n",
    "#     Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "#     inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "#     output: shape=[dim_1, ..., dim_{n-1}]\n",
    "#     \"\"\"\n",
    "#     def call(self, inputs, **kwargs):\n",
    "#         return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return input_shape[:-1]\n",
    "\n",
    "# class Mask(layers.Layer):\n",
    "#     \"\"\"\n",
    "#     Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
    "#     Output shape: [None, d2]\n",
    "#     \"\"\"\n",
    "#     def call(self, inputs, **kwargs):\n",
    "#         # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
    "#         if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
    "#             assert len(inputs) == 2\n",
    "#             inputs, mask = inputs\n",
    "#         else:  # if no true label, mask by the max length of vectors of capsules\n",
    "#             x = inputs\n",
    "#             # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
    "#             x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
    "#             mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
    "\n",
    "#         # masked inputs, shape = [batch_size, dim_vector]\n",
    "#         inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
    "#         return inputs_masked\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         if type(input_shape[0]) is tuple:  # true label provided\n",
    "#             return tuple([None, input_shape[0][-1]])\n",
    "#         else:\n",
    "#             return tuple([None, input_shape[-1]])\n",
    "\n",
    "\n",
    "# def squash(vectors, axis=-1):\n",
    "#     \"\"\"\n",
    "#     The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "#     :param vectors: some vectors to be squashed, N-dim tensor\n",
    "#     :param axis: the axis to squash\n",
    "#     :return: a Tensor with same shape as input vectors\n",
    "#     \"\"\"\n",
    "#     s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "#     scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
    "#     return scale * vectors\n",
    "\n",
    "\n",
    "# class CapsuleLayer(layers.Layer):\n",
    "#     \"\"\"\n",
    "#     The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "#     neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "#     from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "#     [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    \n",
    "#     :param num_capsule: number of capsules in this layer\n",
    "#     :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "#     :param num_routings: number of iterations for the routing algorithm\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "#                  kernel_initializer='glorot_uniform',\n",
    "#                  bias_initializer='zeros',\n",
    "#                  **kwargs):\n",
    "#         super(CapsuleLayer, self).__init__(**kwargs)\n",
    "#         self.num_capsule = num_capsule\n",
    "#         self.dim_vector = dim_vector\n",
    "#         self.num_routing = num_routing\n",
    "#         self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "#         self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "#         self.input_num_capsule = input_shape[1]\n",
    "#         self.input_dim_vector = input_shape[2]\n",
    "\n",
    "#         # Transformation matrix/Weight matrix\n",
    "#         self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "#                                  initializer=self.kernel_initializer,\n",
    "#                                  name='W')\n",
    "\n",
    "#         # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "#         self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "#                                     initializer=self.bias_initializer,\n",
    "#                                     name='bias',\n",
    "#                                     trainable=False)\n",
    "#         self.built = True\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "# #     def call(self, inputs, training=None):\n",
    "# #         # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "# #         # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "# #         inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "# #         # Replicate(tile) num_capsule dimension to prepare being multiplied by W\n",
    "# #         # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "# #         inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "# #         \"\"\"  \n",
    "# #         # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "# #         # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "# #         w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        \n",
    "# #         # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "# #         inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "# #         \"\"\"\n",
    "        \n",
    "# #         # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "# #         inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "# #                              elems=inputs_tiled,\n",
    "# #                              initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "\n",
    "# #         # DYNAMIC ROUTING\n",
    "# #         assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "# #         for i in range(self.num_routing):\n",
    "# #             c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "# #             outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "# #             # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "# #             if i != self.num_routing - 1:\n",
    "# #                 # update the raw weights for the next routing iteration\n",
    "# #                 # by adding the agreement to the previous raw weights\n",
    "# #                 self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "\n",
    "# #         return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#             return tuple([None, self.num_capsule, self.dim_vector])\n",
    "\n",
    "\n",
    "# def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "#     \"\"\"\n",
    "#     Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "#     :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "#     :param dim_vector: the dim of the output vector of capsule\n",
    "#     :param n_channels: the number of types of capsules\n",
    "#     :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "#     \"\"\"\n",
    "#     output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding, name='primary_caps_conv')(inputs)\n",
    "#     outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primary_caps_reshape')(output)\n",
    "#     return layers.Lambda(squash)(outputs)\n",
    "\n",
    "\n",
    "\n",
    "# from keras import layers, models\n",
    "# from keras import backend as K\n",
    "# from keras.utils import to_categorical\n",
    "# def CapsNet(input_shape, n_class, num_routing):\n",
    "#     \"\"\"\n",
    "#     A Capsule Network on MNIST.\n",
    "#     :param input_shape: data shape, 4d, [None, width, height, channels]\n",
    "#     :param n_class: number of classes\n",
    "#     :param num_routing: number of routing iterations\n",
    "#     :return: A Keras Model with 2 inputs and 2 outputs\n",
    "#     \"\"\"\n",
    "#     x = layers.Input(shape=input_shape)\n",
    "\n",
    "#     # Changes:\n",
    "#     # Added conv layer before feeding to primary caps layer, with 256 kernels\n",
    "\n",
    "#     # Layer 1: Conventional Conv2D layer 1\n",
    "#     conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "#     # Layer 2: Conventional Conv2D layer 2\n",
    "#     conv2 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv2')(conv1)\n",
    "\n",
    "#     # Layer 3: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "#     primarycaps = PrimaryCap(conv2, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "#     # Layer 4: Capsule layer. Dynamic Routing algorithm works here.\n",
    "#     digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digit_caps')(primarycaps)\n",
    "\n",
    "#     # Layer 5: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "#     out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "#     # Decoder network.\n",
    "#     y = layers.Input(shape=(n_class,))\n",
    "#     masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
    "#     x_recon = layers.Dense(512, activation='relu')(masked)\n",
    "#     x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
    "#     x_recon = layers.Dense(784, activation='sigmoid')(x_recon)\n",
    "#     x_recon = layers.Reshape(target_shape=[28, 28, 1], name='out_recon')(x_recon)\n",
    "\n",
    "#     # two-input-two-output keras Model\n",
    "#     return models.Model([x, y], [out_caps, x_recon])\n",
    "\n",
    "\n",
    "# def margin_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "#     :param y_true: [None, n_classes]\n",
    "#     :param y_pred: [None, num_capsule]\n",
    "#     :return: a scalar loss value.\n",
    "#     \"\"\"\n",
    "#     L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "#         0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "#     return K.mean(K.sum(L, 1))\n",
    "\n",
    "\n",
    "# num_classes = 3\n",
    "# routings = 3\n",
    "\n",
    "# # define model\n",
    "# model = CapsNet(input_shape=[28, 28, 1],\n",
    "#                 n_class=num_classes,\n",
    "#                 num_routing=routings)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Length, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "#         self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "\n",
    "#         self.kernel_initializer = initializers.random_uniform(-1, 1)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "    def call(self, inputs, training=None):\n",
    "        # Expand the input in axis=1, tile in that axis to num_capsule, and \n",
    "        #  expands another axis at the end to prepare the multiplication with W.\n",
    "        #  inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        #  inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        #  inputs_tiled.shape=[None, num_capsule, input_num_capsule, \n",
    "        #                            input_dim_capsule, 1]\n",
    "        inputs_expand = tf.expand_dims(inputs, 1)\n",
    "        inputs_tiled  = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "        inputs_tiled  = tf.expand_dims(inputs_tiled, 4)\n",
    "\n",
    "        # Compute `W * inputs` by scanning inputs_tiled on dimension 0 (map_fn).\n",
    "        # - Use matmul (without transposing any element). Note the order!\n",
    "        # Thus:\n",
    "        #  x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
    "        #  W.shape=[num_capsule, input_num_capsule, dim_capsule,input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [dim_capsule, input_dim_capsule] x [input_dim_capsule, 1]-> \n",
    "        #              [dim_capsule, 1].\n",
    "        #  inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]\n",
    "\n",
    "        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled)     \n",
    "\n",
    "        # Begin: Routing algorithm ----------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        #  b.shape = [None, self.num_capsule, self.input_num_capsule, 1, 1].\n",
    "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.num_capsule,  self.input_num_capsule, 1, 1])\n",
    "\n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "                # Apply softmax to the axis with `num_capsule`\n",
    "                #  c.shape=[batch_size, num_capsule, input_num_capsule, 1, 1]\n",
    "                c = layers.Softmax(axis=1)(b)\n",
    "\n",
    "                # Compute the weighted sum of all the predicted output vectors.\n",
    "                #  c.shape =  [batch_size, num_capsule, input_num_capsule, 1, 1]\n",
    "                #  inputs_hat.shape=[None, num_capsule, input_num_capsule,dim_capsule,1]\n",
    "                # The function `multiply` will broadcast axis=3 in c to dim_capsule.\n",
    "                #  outputs.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]\n",
    "                # Then sum along the input_num_capsule\n",
    "                #  outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
    "                # Then apply squash along the dim_capsule\n",
    "                outputs = tf.multiply(c, inputs_hat)\n",
    "                outputs = tf.reduce_sum(outputs, axis=2, keepdims=True)\n",
    "                outputs = squash(outputs, axis=-2)  # [None, 10, 1, 16, 1]\n",
    "\n",
    "                if i < self.routings - 1:\n",
    "                      # Update the prior b.\n",
    "                      #  outputs.shape =  [None, num_capsule, 1, dim_capsule, 1]\n",
    "                      #  inputs_hat.shape=[None,num_capsule,input_num_capsule,dim_capsule,1]\n",
    "                      # Multiply the outputs with the weighted_inputs (inputs_hat) and add  \n",
    "                      # it to the prior b.  \n",
    "                        outputs_tiled = tf.tile(outputs, [1, 1, self.input_num_capsule, 1, 1])\n",
    "                        agreement = tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\n",
    "                        b = tf.add(b, agreement)\n",
    "\n",
    "        # End: Routing algorithm ------------------------------------------------#\n",
    "        # Squeeze the outputs to remove useless axis:\n",
    "        #  From  --> outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
    "        #  To    --> outputs.shape=[None, num_capsule,    dim_capsule]\n",
    "        outputs = tf.squeeze(outputs, [2, 4])\n",
    "        return outputs\n",
    "#     def call(self, inputs, training=None):\n",
    "#         # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "#         # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "#         inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "#         # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "#         # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "#         inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "#         # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "#         # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "#         # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "#         # Regard the first two dimensions as `batch` dimension,\n",
    "#         # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "#         # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "#         inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "#         # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "#         # The prior for coupling coefficient, initialized as zeros.\n",
    "#         # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "#         b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "#         assert self.routings > 0, 'The routings should be > 0.'\n",
    "#         for i in range(self.routings):\n",
    "#             # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "#             c = tf.nn.softmax(b, axis=1)\n",
    "\n",
    "#             # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "#             # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "#             # The first two dimensions as `batch` dimension,\n",
    "#             # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "#             # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "#             outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "\n",
    "#             if i < self.routings - 1:\n",
    "#                 # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "#                 # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "#                 # The first two dimensions as `batch` dimension,\n",
    "#                 # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "#                 # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "#                 b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "#         # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "#         return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_capsule': self.num_capsule,\n",
    "            'dim_capsule': self.dim_capsule,\n",
    "            'routings': self.routings\n",
    "        }\n",
    "        base_config = super(CapsuleLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    \n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "def CapsNet(input_shape, classes, routings=3):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "#     conv1 = layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "#     conv1 = layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv2')(conv1)\n",
    "#     conv1 = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv3')(conv1)\n",
    "    \n",
    "    \n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    conv1 = layers.BatchNormalization()(conv1)\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "    primarycaps = layers.BatchNormalization()(primarycaps)\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=classes, dim_capsule=16, routings=routings,\n",
    "                             name='digitcaps')(primarycaps)\n",
    "    digitcaps = layers.BatchNormalization()(digitcaps)\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(classes,))\n",
    "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(layers.Dense(512, activation='relu', input_dim=16*classes))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    \n",
    "    decoder.add(layers.Dense(1024, activation='relu'))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "\n",
    "    # manipulate model\n",
    "    noise = layers.Input(shape=(classes, 16))\n",
    "    noised_digitcaps = layers.Add()([digitcaps, noise])\n",
    "    masked_noised_y = Mask()([noised_digitcaps, y])\n",
    "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
    "    return train_model, eval_model, manipulate_model\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n",
    "\n",
    "\n",
    "# def train(model, data, args):\n",
    "#     \"\"\"\n",
    "#     Training a CapsuleNet\n",
    "#     :param model: the CapsuleNet model\n",
    "#     :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "#     :param args: arguments\n",
    "#     :return: The trained model\n",
    "#     \"\"\"\n",
    "#     # unpacking the data\n",
    "#     (x_train, y_train), (x_test, y_test) = data\n",
    "\n",
    "#     # callbacks\n",
    "# #     log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
    "# #     tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs',\n",
    "# #                                batch_size=args.batch_size, histogram_freq=int(args.debug))\n",
    "#     checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}.h5', monitor='val_capsnet_acc',\n",
    "#                                            save_best_only=True, save_weights_only=True, verbose=1)\n",
    "#     lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
    "\n",
    "#     # compile the model\n",
    "#     model.compile(optimizer=optimizers.Adam(lr=args.lr),\n",
    "#                   loss=[margin_loss, 'mse'],\n",
    "#                   loss_weights=[1., args.lam_recon],\n",
    "#                   metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Training without data augmentation:\n",
    "#     model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "#               validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "#     def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "#         train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "#                                            height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n",
    "#         generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "#         while 1:\n",
    "#             x_batch, y_batch = generator.next()\n",
    "#             yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "#     # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "#     model.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n",
    "#                         steps_per_epoch=int(y_train.shape[0] / args.batch_size),\n",
    "#                         epochs=args.epochs,\n",
    "#                         validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "#                         callbacks=[checkpoint, lr_decay])\n",
    "#     # End: Training with data augmentation -----------------------------------------------------------------------#\n",
    "\n",
    "#     model.save_weights(args.save_dir + '/trained_model.h5')\n",
    "#     print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n",
    "\n",
    "#     from utils import plot_log\n",
    "#     plot_log(args.save_dir + '/log.csv', show=True)\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def test(model, data, args):\n",
    "#     x_test, y_test = data\n",
    "#     y_pred, x_recon = model.predict(x_test, batch_size=100)\n",
    "#     print('-'*30 + 'Begin: test' + '-'*30)\n",
    "#     print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "#     img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "#     image = img * 255\n",
    "#     Image.fromarray(image.astype(np.uint8)).save(args.save_dir + \"/real_and_recon.png\")\n",
    "#     print()\n",
    "#     print('Reconstructed images are saved to %s/real_and_recon.png' % args.save_dir)\n",
    "#     print('-' * 30 + 'End: test' + '-' * 30)\n",
    "#     plt.imshow(plt.imread(args.save_dir + \"/real_and_recon.png\"))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def manipulate_latent(model, data, args):\n",
    "#     print('-'*30 + 'Begin: manipulate' + '-'*30)\n",
    "#     x_test, y_test = data\n",
    "#     index = np.argmax(y_test, 1) == args.digit\n",
    "#     number = np.random.randint(low=0, high=sum(index) - 1)\n",
    "#     x, y = x_test[index][number], y_test[index][number]\n",
    "#     x, y = np.expand_dims(x, 0), np.expand_dims(y, 0)\n",
    "#     noise = np.zeros([1, 10, 16])\n",
    "#     x_recons = []\n",
    "#     for dim in range(16):\n",
    "#         for r in [-0.25, -0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "#             tmp = np.copy(noise)\n",
    "#             tmp[:,:,dim] = r\n",
    "#             x_recon = model.predict([x, y, tmp])\n",
    "#             x_recons.append(x_recon)\n",
    "\n",
    "#     x_recons = np.concatenate(x_recons)\n",
    "\n",
    "#     img = combine_images(x_recons, height=16)\n",
    "#     image = img*255\n",
    "#     Image.fromarray(image.astype(np.uint8)).save(args.save_dir + '/manipulate-%d.png' % args.digit)\n",
    "#     print('manipulated result saved to %s/manipulate-%d.png' % (args.save_dir, args.digit))\n",
    "#     print('-' * 30 + 'End: manipulate' + '-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleDataGenerator(keras.utils.Sequence):\n",
    "        'Generates data for Keras'\n",
    "        def __init__(self, list_IDs, labels, batch_size=32, dim=(256, 256), n_channels=1,\n",
    "                     n_classes=10, shuffle=True, resize_setting=(256, 256)):\n",
    "                'Initialization'\n",
    "                self.dim = dim\n",
    "                self.batch_size = batch_size\n",
    "                self.labels = labels\n",
    "                self.list_IDs = list(list_IDs)\n",
    "                self.n_channels = n_channels\n",
    "                self.n_classes = n_classes\n",
    "                self.shuffle = shuffle\n",
    "                self.on_epoch_end()\n",
    "                self.resize_setting = resize_setting\n",
    "                \n",
    "\n",
    "        def __len__(self):\n",
    "                'Denotes the number of batches per epoch'\n",
    "                return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "                'Generate one batch of data'\n",
    "                # Generate indexes of the batch\n",
    "                end = (index+1)*self.batch_size if (index+1)*self.batch_size < len(self.indexes) else len(self.indexes)\n",
    "                indexes = self.indexes[index*self.batch_size:end] \n",
    "\n",
    "                # Find list of IDs\n",
    "                list_IDs_temp = [self.list_IDs[k] for k in indexes]         \n",
    "                # Generate data\n",
    "                X, y = self.__data_generation(list_IDs_temp)\n",
    "               \n",
    "                return X, y\n",
    "\n",
    "        def on_epoch_end(self):\n",
    "                'Updates indexes after each epoch'\n",
    "                self.indexes = np.arange(len(self.list_IDs))\n",
    "                if self.shuffle == True:\n",
    "                        np.random.shuffle(self.indexes)\n",
    "        \n",
    "        def noisy_image(self, noise_typ, image):\n",
    "                if noise_typ == \"gauss\":\n",
    "                          row,col= image.shape\n",
    "                          mean = 0\n",
    "                          var = 0.1\n",
    "                          sigma = var**0.5\n",
    "                          gauss = np.random.normal(mean,sigma,(row,col))\n",
    "                          gauss = gauss.reshape(row,col)\n",
    "                          noisy = image + gauss\n",
    "                          return noisy\n",
    "\n",
    "                elif noise_typ == \"s&p\":\n",
    "                          row,col = image.shape\n",
    "                          s_vs_p = 0.5\n",
    "                          amount = 0.004\n",
    "                          out = np.copy(image)\n",
    "                          # Salt mode\n",
    "                          num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "                          coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "                                  for i in image.shape]\n",
    "                          out[coords] = 1\n",
    "\n",
    "                          # Pepper mode\n",
    "                          num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "                          coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "                                  for i in image.shape]\n",
    "                          out[coords] = 0\n",
    "                          return out\n",
    "\n",
    "                elif noise_typ == \"poisson\":\n",
    "                          vals = len(np.unique(image))\n",
    "                          vals = 2 ** np.ceil(np.log2(vals))\n",
    "                          noisy = np.random.poisson(image * vals) / float(vals)\n",
    "                          return noisy\n",
    "\n",
    "                elif noise_typ == \"speckle\":\n",
    "                          row,col = image.shape\n",
    "                          gauss = np.random.randn(row,col)\n",
    "                          gauss = gauss.reshape(row,col)        \n",
    "                          noisy = image + image * gauss\n",
    "                          return noisy\n",
    "            \n",
    "        def __data_generation(self, list_IDs_temp):\n",
    "            \n",
    "                # Initialization\n",
    "                image_array_size = ( self.batch_size, *self.dim, self.n_channels )\n",
    "                X = np.zeros(image_array_size, dtype=np.uint8)\n",
    "                y = np.array([0]* self.batch_size)\n",
    "                \n",
    "                for i, ID in enumerate(list_IDs_temp):\n",
    "                        image = cv2.imread(ID, 0)\n",
    "                        image = image.astype('float32') / 255\n",
    "                        image = cv2.resize(image, (self.resize_setting[1], self.resize_setting[0]))\n",
    "                        \n",
    "#                         from random import sample\n",
    "#                         random_index = sample(range(0, 32), k=8)\n",
    "                \n",
    "#                         if i in random_index: image = self.noisy_image(\"gauss\", image)\n",
    "                        image = np.reshape(image, (*image.shape, 1))\n",
    "                        X[i,], y[i] =  image, self.labels[ID]\n",
    "                        \n",
    "                        x_batch, y_batch = X, to_categorical(y, num_classes=self.n_classes)\n",
    "                return ([x_batch, y_batch], [y_batch, x_batch])\n",
    "            \n",
    "def make_cap_generator(dataset, batch_size=32):\n",
    "        common_params = {\n",
    "                              'batch_size': batch_size,\n",
    "                              'n_classes' : len(np.unique(dataset.Class)),\n",
    "                              'n_channels': 1,\n",
    "                              'shuffle'   : False,\n",
    "                              'resize_setting': (100, 90),\n",
    "                              'dim': (100, 90)\n",
    "        }\n",
    "        \n",
    "        dataset_dict = collections.OrderedDict(zip(dataset.Path, dataset.Class))\n",
    "        dataset_generator = CapsuleDataGenerator(dataset[\"Path\"], dataset_dict ,**common_params)\n",
    "        return dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 0: 58560, stage 1: 105040, stage 2: 46080, stage 3: 19120\n",
      "stage 0: 19520, stage 1: 34960, stage 2: 15040, stage 3: 6240\n",
      "stage 0: 19520, stage 1: 35200, stage 2: 15440, stage 3: 6720\n",
      "Training dataset: 57360,Validation dataset: 18720, Testing dataset: 20160\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "Fold 1: loading Capsnet ......\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Inconsistent shapes: saw (40128, 3, 1, 3, 16) but expected (40128, 3, 1, 16) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-035b8a7a2d94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanipulate_model\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel_fnc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Class\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         model.compile(optimizer=optimizers.Adam(lr=0.01),\n",
      "\u001b[1;32mD:\\Dental_Project\\Dental_Tool\\Capsule.py\u001b[0m in \u001b[0;36mCapsNet\u001b[1;34m(input_shape, classes, num_routing)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m# Layer 4: Capsule layer. Dynamic Routing algorithm works here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[0mdigitcaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCapsuleLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_capsule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_routing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_routing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'digit_caps'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimarycaps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;31m# Layer 5: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Dental_Project\\Dental_Tool\\Capsule.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    117\u001b[0m         inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n\u001b[0;32m    118\u001b[0m                              \u001b[0melems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_tiled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                              initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;31m# DYNAMIC ROUTING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mscan\u001b[1;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         maximum_iterations=n)\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2673\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[0mreturn_same_structure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_same_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m         back_prop=back_prop)\n\u001b[0m\u001b[0;32m   2676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"while\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, maximum_iterations, name, return_same_structure, back_prop)\u001b[0m\n\u001b[0;32m    196\u001b[0m         func_graph=util.WhileBodyFuncGraph(\n\u001b[0;32m    197\u001b[0m             body_name, collections=ops.get_default_graph()._collections),  # pylint: disable=protected-access\n\u001b[1;32m--> 198\u001b[1;33m         add_control_dependencies=add_control_dependencies)\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[1;31m# Add external captures of body to the list of loop vars.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;31m# Note that external tensors will be treated as loop invariants, i.e.,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py\u001b[0m in \u001b[0;36mwrapped_body\u001b[1;34m(loop_counter, maximum_iterations_arg, *args)\u001b[0m\n\u001b[0;32m    174\u001b[0m       \u001b[1;31m# `orig_loop_vars` and `args`, converts flows in `args` to TensorArrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[1;31m# and packs it into the structure of `orig_loop_vars`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_pack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_loop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(i, a_flat, tas)\u001b[0m\n\u001b[0;32m    487\u001b[0m                                  a_out)\n\u001b[0;32m    488\u001b[0m       \u001b[0mflat_a_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m       \u001b[0mtas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_a_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mnext_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    487\u001b[0m                                  a_out)\n\u001b[0;32m    488\u001b[0m       \u001b[0mflat_a_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m       \u001b[0mtas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_a_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mnext_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\tensor_array_ops.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, index, value, name)\u001b[0m\n\u001b[0;32m   1082\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mwriters\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mspecified\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \"\"\"\n\u001b[1;32m-> 1084\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_implementation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m   \"\"\"\n\u001b[0;32m    197\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_add_should_use_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m   return tf_decorator.make_decorator(\n\u001b[0;32m    200\u001b[0m       \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'should_use_result'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\tensor_array_ops.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, index, value, name)\u001b[0m\n\u001b[0;32m    524\u001b[0m           value, preferred_dtype=self._dtype, name=\"value\")\n\u001b[0;32m    525\u001b[0m       \u001b[0m_check_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_element_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m       flow_out = list_ops.tensor_list_set_item(\n\u001b[0;32m    528\u001b[0m           \u001b[0minput_handle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\lawrence\\lib\\site-packages\\tensorflow_core\\python\\ops\\tensor_array_ops.py\u001b[0m in \u001b[0;36m_check_element_shape\u001b[1;34m(self, shape)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m       raise ValueError(\"Inconsistent shapes: saw %s but expected %s \" %\n\u001b[1;32m--> 494\u001b[1;33m                        (shape, self.element_shape))\n\u001b[0m\u001b[0;32m    495\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Inconsistent shapes: saw (40128, 3, 1, 3, 16) but expected (40128, 3, 1, 16) "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "\n",
    "# from Dental_Tool.Inception_v3 import Custom_Inception_V3, Simple_Inception\n",
    "# from Dental_Tool.Inception import create_inception_v4\n",
    "from Dental_Tool.Data_processing import *\n",
    "# from Dental_Tool.Dental_Model import *\n",
    "from Dental_Tool.Process_results import *\n",
    "from Dental_Tool.Dataloader import *\n",
    "from Dental_Tool.KFold_v2 import *\n",
    "\n",
    "import keras.backend as K \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import time\n",
    "# from Dental_Tool.Capsule import CapsNet\n",
    "\n",
    "root_dir  =  \"Capsnet_Fold_5__Testtttt___________\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "init_directory(f\"Results/{root_dir}/table\")\n",
    "model_info = (\"Capsnet\", CapsNet)\n",
    "performance_list = []\n",
    "epochs     = 5\n",
    "Fold_num = 5\n",
    "num_params = 1\n",
    "fold_data_num = 1\n",
    "\n",
    "        \n",
    "        \n",
    "for train_dataset, valid_dataset, test_dataset, train_generator, valid_generator, test_generator in\\\n",
    "        K_Fold_balance_data_generator(dataset, argscale=argscale_num, k_fold_num=Fold_num):\n",
    "        \n",
    "        cap_train_gen = make_cap_generator(train_dataset, 64)\n",
    "        cap_valid_gen = make_cap_generator(valid_dataset, 64)\n",
    "        cap_test_gen  = make_cap_generator(test_dataset , 64)\n",
    "        \n",
    "        \n",
    "        K.clear_session()\n",
    "        print(f\"Training dataset: {len(train_dataset)},Validation dataset: {len(valid_dataset)}, Testing dataset: {len(test_dataset)}\" )\n",
    "        \n",
    "        init_directory(f\"Results/{root_dir}/Fold_{fold_data_num}/table\")\n",
    "           \n",
    "        train_dataset.to_csv(f\"Results/{root_dir}/Fold_{fold_data_num}/table/train_dataset.csv\", index=False)\n",
    "        valid_dataset.to_csv(f\"Results/{root_dir}/Fold_{fold_data_num}/table/valid_dataset.csv\", index=False)\n",
    "        test_dataset.to_csv(f\"Results/{root_dir}/Fold_{fold_data_num}/table/test_dataset.csv\"  , index=False)\n",
    "        \n",
    "        print(set(train_dataset.source) & set(valid_dataset.source))\n",
    "        print(set(valid_dataset.source) & set(test_dataset.source))\n",
    "        print(set(train_dataset.source) & set(test_dataset.source))\n",
    "        \n",
    "        K.clear_session()\n",
    "        best_models = None\n",
    "        \n",
    "        model_name, model_fnc = model_info\n",
    "        \n",
    "        K.clear_session()\n",
    "\n",
    "        param_dir  = f\"{model_name}_Fold_{fold_data_num}\"\n",
    "        print(f\"Fold {fold_data_num}: loading {model_name} ......\")\n",
    "\n",
    "        start = time.time()\n",
    "        model, eval_model, manipulate_model  = model_fnc(input_shape=(100, 90, 1), classes=len(np.unique(train_dataset[\"Class\"])))\n",
    "        \n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                      loss=[margin_loss, 'mse'],\n",
    "                      loss_weights=[ 1, 0.005],\n",
    "                      metrics={'capsnet': 'accuracy'})\n",
    "        model.summary()\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        print(f\"Fold {fold_data_num}: loading Done, cost {elapse} seconds\")\n",
    "\n",
    "\n",
    "        init_directory(f\"Results/{root_dir}/Fold_{fold_data_num}/{param_dir}/parameter\")\n",
    "\n",
    "        filepath=\"Results/%s/Fold_%d/%s/parameter/weights-improvement-{epoch:02d}-{val_accuracy:.4f}.hdf5\" \\\n",
    "                    %(root_dir, fold_data_num, param_dir)\n",
    "        \n",
    "        from keras import callbacks\n",
    "        checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "        lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "        # change \n",
    "        history = model.fit_generator(\n",
    "                                generator=cap_train_gen,\n",
    "                                validation_data=cap_valid_gen,\n",
    "                                epochs=epochs,\n",
    "                                workers=2,\n",
    "                                shuffle=True,\n",
    "                                verbose=1,\n",
    "                                callbacks=[ checkpoint, lr_decay ],\n",
    "                               )\n",
    "        \n",
    "        plot_result(history, f\"Results/{root_dir}/Fold_{fold_data_num}/{param_dir}\")\n",
    "\n",
    "        K.clear_session()\n",
    "        top_k_param = get_k_top_value(f\"Results/{root_dir}/Fold_{fold_data_num}/{param_dir}/parameter\",  k_th = num_params )\n",
    "        best_param = top_k_param[0]\n",
    "        \n",
    "        K.clear_session()\n",
    "        print(f\"loading {best_param} ......\")\n",
    "        start = time.time()\n",
    "        best_model = load_model(best_param)\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        print(f\"loading Done, cost {elapse} seconds\")\n",
    "\n",
    "        test_predict, rec_image = best_model.predict_generator(cap_test_gen, verbose=1)\n",
    "        test_predict = test_predict[:len(test_dataset)]\n",
    "        \n",
    "        test_result  = np.argmax(test_predict, axis=1)\n",
    "        test_dataset[\"Predict\"] = test_result\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        confusion = confusion_matrix(test_dataset.Class.astype('int'), test_result.astype('int'))\n",
    "        curr_acc = (test_dataset.Class == test_dataset.Predict).mean() * 100\n",
    "        title = \"Accuracy  = {:5.2f} %\".format(curr_acc)\n",
    "        print(title)\n",
    "        init_directory(f\"Results/{root_dir}/Fold_{fold_data_num}/table\")\n",
    "        plot_confusion_matrix(confusion, len(np.unique(test_dataset.Class)), curr_acc, f\"Results/{root_dir}/Fold_{fold_data_num}\", 0)\n",
    "        \n",
    "        test_predict = np.round(test_predict, 2)\n",
    "        test_dataset[f\"{model_name}_Prob\"] = pd.Series(test_predict.tolist())\n",
    "        test_dataset[\"Predict\"] = test_result\n",
    "        \n",
    "        test_dataset.to_csv(f\"Results/{root_dir}/Fold_{fold_data_num}/table/test_predict.csv\", index=False)\n",
    "        print(statistic(test_dataset))\n",
    "        K.clear_session()\n",
    "        \n",
    "        performance_list.append(curr_acc)\n",
    "        fold_data_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
