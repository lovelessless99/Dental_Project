{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from Dental_Tool.Data_processing import *\n",
    "from Dental_Tool.Dental_Model import *\n",
    "from Dental_Tool.Process_results import *\n",
    "from Dental_Tool.Dataloader import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = [ \n",
    "                \"Dental_Data/PBL/10_interdental_20201015_multilabel_max4\", \n",
    "                \"Dental_Data/PBL/10_interdental_20201015_multilabel_max4_flip\", \n",
    "                \"Dental_Data/PBL/10_interdental_20201015_clahe_multilabel_max4\", \n",
    "                \"Dental_Data/PBL/10_interdental_20201015_clahe_multilabel_max4_flip\"\n",
    "            ]\n",
    "\n",
    "directory = [ i + \"/mapping.json\" for i in directory]\n",
    "argscale_num = len(directory) * 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(data_list, interdental=False):\n",
    "            filter_fun = lambda x : { path: max(list(map(int, state))) for path, state in x.items() if max(list(map(int, state))) >= 0 }\n",
    "               \n",
    "            interdental_fun = lambda x : { path: state for path, state in x.items() }\n",
    "            \n",
    "            results = collections.OrderedDict()\n",
    "            all_filtering_data, all_keys = [], []\n",
    "            \n",
    "            for dataset_path in data_list:\n",
    "                        mapping_data = json.load(open(dataset_path , \"r\"))\n",
    "                        filter_data  = filter_fun(mapping_data) if not interdental else interdental_fun(mapping_data)\n",
    "                        all_filtering_data.append(filter_data)\n",
    "                        all_keys.append( list(filter_data.keys()) )\n",
    "            \n",
    "            for keys in zip(*all_keys):\n",
    "                    for key,  data in zip(keys, all_filtering_data):\n",
    "                            results[key] = data[key]        \n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_2_dataframe_PBL_inderdental(data, mode=None):\n",
    "        PBL_Columns = [\"Path\", \"State\", \"Class\", \"bone_loss\", \"furcation\", \"buccal_furcation\", \"endo_lesion\"]\n",
    "        \n",
    "        dataframe = pd.DataFrame(columns=PBL_Columns)\n",
    "        data_dict, counter = collections.OrderedDict(), 0\n",
    "        \n",
    "        molar    = [1, 2, 3, 14, 15, 16, 17, 18, 19, 30, 31, 32]\n",
    "        premolar = [ 4, 5, 12, 13, 20, 21, 28, 29 ]\n",
    "        canine   = [ 6, 11, 22, 27                 ]\n",
    "        incisor  = [ 7, 8 , 9 , 10, 23, 24, 25, 26 ]\n",
    "        all_molar = molar + premolar\n",
    "       \n",
    "        less_data  = [1, 16, 17, 32]\n",
    "        \n",
    "        for path, info in data.items():\n",
    "                state = info[\"state\"]\n",
    "                item = { \n",
    "                          \"Path\": path,\n",
    "                          \"State\": state,\n",
    "                          \"Class\": state-1 if state > 1 else 0,\n",
    "                          **info\n",
    "                }\n",
    "                \n",
    "                path_split = path.split(\"_\")\n",
    "                \n",
    "#                 in_dir = path.split(\"/\")[2]\n",
    "\n",
    "                NN_IDX = 0\n",
    "                for idx, i in enumerate(path_split):\n",
    "                        if i == \"NN\":\n",
    "                            NN_IDX = idx\n",
    "                            break\n",
    "                \n",
    "                Patrica_IDX = 0\n",
    "                if NN_IDX == 0:\n",
    "                        for idx, i in enumerate(path_split):\n",
    "                                if \"Patrica\" in i:\n",
    "                                    Patrica_IDX = idx\n",
    "                                    break\n",
    "                    \n",
    "                    \n",
    "                original, source = \"\", \"\"\n",
    "                \n",
    "                if NN_IDX == 0:\n",
    "                        source = \"_\".join(path_split[-7:-2]) \n",
    "                        original = \"_\".join(path_split[-7:-3])\n",
    "                \n",
    "                else: \n",
    "                        source = \"_\".join(path_split[NN_IDX:-2])\n",
    "                        original = \"_\".join(path_split[NN_IDX:-3])\n",
    "                \n",
    "                if NN_IDX != 0:\n",
    "                        if ' ' == path_split[NN_IDX-1][1]:  ID = \"_\".join(path_split[NN_IDX-2:NN_IDX])\n",
    "                        else  : ID = path_split[NN_IDX-1]\n",
    "                \n",
    "                else: \n",
    "                        if ' ' == path_split[Patrica_IDX-1][1]:  ID = \"_\".join(path_split[Patrica_IDX-2:Patrica_IDX])\n",
    "                        else  : ID = path_split[Patrica_IDX-1]\n",
    "                \n",
    "                item[\"ID\"] = ID\n",
    "                \n",
    "                item[\"tooth_num\"] = int(path_split[-3])\n",
    "                item[\"ori_src\"] = original\n",
    "                item[\"source\"] = source\n",
    "                item[\"side\"] = source + \"_\" + path[-5]\n",
    "\n",
    "                \n",
    "                if item[\"tooth_num\"] in molar     : item[\"tooth_type\"] = 0\n",
    "                elif item[\"tooth_num\"] in premolar: item[\"tooth_type\"] = 1\n",
    "                elif item[\"tooth_num\"] in canine  : item[\"tooth_type\"] = 2\n",
    "                elif item[\"tooth_num\"] in incisor : item[\"tooth_type\"] = 3\n",
    "                else : item[\"tooth_type\"] = -99\n",
    "                    \n",
    "                item[\"side\"] = source + \"_\" + path[-5]\n",
    "                \n",
    "                cond_1 = (mode == \"molar\"    ) and (item[\"tooth_num\"] not in molar    )\n",
    "                cond_2 = (mode == \"premolar\" ) and (item[\"tooth_num\"] not in premolar )\n",
    "                cond_3 = (mode == \"canine\"   ) and (item[\"tooth_num\"] not in canine   )\n",
    "                cond_4 = (mode == \"incisor\"  ) and (item[\"tooth_num\"] not in incisor  )\n",
    "                cond_5 = (mode == \"all_molar\") and (item[\"tooth_num\"] not in all_molar)\n",
    "                \n",
    "                if cond_1 or cond_2 or cond_3 or cond_4 or cond_5: continue\n",
    "                    \n",
    "                item[\"angle\"] = int(path_split[-2].split(\".\")[0])\n",
    "                \n",
    "                data_dict[counter] = item\n",
    "                counter += 1\n",
    "        dataframe = dataframe.from_dict(data_dict, \"index\")\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>State</th>\n",
       "      <th>Class</th>\n",
       "      <th>state</th>\n",
       "      <th>bone_loss</th>\n",
       "      <th>furcation</th>\n",
       "      <th>buccal_furcation</th>\n",
       "      <th>endo_lesion</th>\n",
       "      <th>ID</th>\n",
       "      <th>tooth_num</th>\n",
       "      <th>ori_src</th>\n",
       "      <th>source</th>\n",
       "      <th>side</th>\n",
       "      <th>tooth_type</th>\n",
       "      <th>angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_multil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>6</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>2</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_multil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>6</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>2</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>6</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>2</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>6</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>2</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_multil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-999</td>\n",
       "      <td>0</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>6</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_R</td>\n",
       "      <td>2</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776155</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_clahe_...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>26</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_L</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776156</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_multil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>26</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776157</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_multil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>26</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776158</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>26</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776159</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_20201015_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>26</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630000 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Path  State  Class  \\\n",
       "0       Dental_Data/PBL/10_interdental_20201015_multil...      1      0   \n",
       "1       Dental_Data/PBL/10_interdental_20201015_multil...      1      0   \n",
       "2       Dental_Data/PBL/10_interdental_20201015_clahe_...      1      0   \n",
       "3       Dental_Data/PBL/10_interdental_20201015_clahe_...      1      0   \n",
       "4       Dental_Data/PBL/10_interdental_20201015_multil...      1      0   \n",
       "...                                                   ...    ...    ...   \n",
       "776155  Dental_Data/PBL/10_interdental_20201015_clahe_...      2      1   \n",
       "776156  Dental_Data/PBL/10_interdental_20201015_multil...      1      0   \n",
       "776157  Dental_Data/PBL/10_interdental_20201015_multil...      1      0   \n",
       "776158  Dental_Data/PBL/10_interdental_20201015_clahe_...      1      0   \n",
       "776159  Dental_Data/PBL/10_interdental_20201015_clahe_...      1      0   \n",
       "\n",
       "        state  bone_loss  furcation  buccal_furcation  endo_lesion  \\\n",
       "0           1          1        -99               -99          -99   \n",
       "1           1          1        -99               -99          -99   \n",
       "2           1          1        -99               -99          -99   \n",
       "3           1          1        -99               -99          -99   \n",
       "4           1          1          0              -999            0   \n",
       "...       ...        ...        ...               ...          ...   \n",
       "776155      2          1        -99               -99          -99   \n",
       "776156      1          1          0                 0            0   \n",
       "776157      1          1          0                 0            0   \n",
       "776158      1          1          0                 0            0   \n",
       "776159      1          1          0                 0            0   \n",
       "\n",
       "                        ID  tooth_num                  ori_src  \\\n",
       "0          000408 102419 x          6  NN_191024_151623_BE78A8   \n",
       "1          000408 102419 x          6  NN_191024_151623_BE78A8   \n",
       "2          000408 102419 x          6  NN_191024_151623_BE78A8   \n",
       "3          000408 102419 x          6  NN_191024_151623_BE78A8   \n",
       "4          000408 102419 x          6  NN_191024_151623_BE78A8   \n",
       "...                    ...        ...                      ...   \n",
       "776155  S594966_2 091718 x         26  NN_180917_113933_C0A0B2   \n",
       "776156  S594966_2 091718 x         26  NN_180917_113933_C0A0B2   \n",
       "776157  S594966_2 091718 x         26  NN_180917_113933_C0A0B2   \n",
       "776158  S594966_2 091718 x         26  NN_180917_113933_C0A0B2   \n",
       "776159  S594966_2 091718 x         26  NN_180917_113933_C0A0B2   \n",
       "\n",
       "                            source                          side  tooth_type  \\\n",
       "0        NN_191024_151623_BE78A8_6   NN_191024_151623_BE78A8_6_L           2   \n",
       "1        NN_191024_151623_BE78A8_6   NN_191024_151623_BE78A8_6_L           2   \n",
       "2        NN_191024_151623_BE78A8_6   NN_191024_151623_BE78A8_6_L           2   \n",
       "3        NN_191024_151623_BE78A8_6   NN_191024_151623_BE78A8_6_L           2   \n",
       "4        NN_191024_151623_BE78A8_6   NN_191024_151623_BE78A8_6_R           2   \n",
       "...                            ...                           ...         ...   \n",
       "776155  NN_180917_113933_C0A0B2_26  NN_180917_113933_C0A0B2_26_L           3   \n",
       "776156  NN_180917_113933_C0A0B2_26  NN_180917_113933_C0A0B2_26_R           3   \n",
       "776157  NN_180917_113933_C0A0B2_26  NN_180917_113933_C0A0B2_26_R           3   \n",
       "776158  NN_180917_113933_C0A0B2_26  NN_180917_113933_C0A0B2_26_R           3   \n",
       "776159  NN_180917_113933_C0A0B2_26  NN_180917_113933_C0A0B2_26_R           3   \n",
       "\n",
       "        angle  \n",
       "0         -10  \n",
       "1         -10  \n",
       "2         -10  \n",
       "3         -10  \n",
       "4         -10  \n",
       "...       ...  \n",
       "776155      9  \n",
       "776156      9  \n",
       "776157      9  \n",
       "776158      9  \n",
       "776159      9  \n",
       "\n",
       "[630000 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_json(directory, interdental=True)\n",
    "dataset = json_2_dataframe_PBL_inderdental(data)\n",
    "\n",
    "dataset = dataset[ (dataset.state >= 0) & (dataset.bone_loss >= 0) ]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000408 102419 x',\n",
       " '000411 112119 x',\n",
       " '001742 082712 x',\n",
       " '002456 060517 x',\n",
       " '002555 042513 x',\n",
       " '003262 103015 x',\n",
       " '003615 010816 x',\n",
       " '003670 020718 x',\n",
       " '004151 091409 x',\n",
       " '004359 030716 x',\n",
       " '004499 110515 x',\n",
       " '005627 120209 x',\n",
       " '007274 021016 x',\n",
       " '007501 082216 x',\n",
       " '008908 090309 x',\n",
       " '010801 011019 x',\n",
       " '010953 031618 x',\n",
       " '019747_1 033011 x',\n",
       " '019747_2 061417 x',\n",
       " '022198_1 080717 x',\n",
       " '022198_2 100919 x',\n",
       " '025179_1 082510 x',\n",
       " '025179_2 101216 x',\n",
       " '026110_1 030513 x',\n",
       " '026110_2 061819 x',\n",
       " '026118_1 110613 x',\n",
       " '026118_2 060718 x',\n",
       " '035015_1 022613 x',\n",
       " '035015_2 082619 x',\n",
       " '043521_1 082113 x',\n",
       " '043521_2 082318 x',\n",
       " '051282_1 032311 x',\n",
       " '051282_2 102115 x',\n",
       " '051282_3 102918 x',\n",
       " '060011_1 033010 x',\n",
       " '060011_2 032817 x',\n",
       " '064768_1 022912 x',\n",
       " '064768_2 062513 x',\n",
       " '068783_1 090309 x',\n",
       " '074670_1 020316 x',\n",
       " '074670_2 071718 x',\n",
       " '077345_1 030719 x',\n",
       " '077345_2 082312 x',\n",
       " '1028067 062718 x',\n",
       " '10689 102418 x',\n",
       " '111084_1 071918 x',\n",
       " '111084_2 021017 x',\n",
       " '13529 092513 x',\n",
       " '1899 120718 x',\n",
       " '415134_1 111210 x',\n",
       " '415134_2 111218 x',\n",
       " '417511 080918 x',\n",
       " '417607 021716 x',\n",
       " '419024_1 110712 x',\n",
       " '419024_2 101817 x',\n",
       " '419477 091415 x',\n",
       " '419790_1 021216 x',\n",
       " '419790_2 061219 x',\n",
       " '420606_1 032409 x',\n",
       " '420606_2 070219 x',\n",
       " '420888 040416 x',\n",
       " '421067 041615 x',\n",
       " '421661 092517 x',\n",
       " '421730 060716 x',\n",
       " '422241 121212 x',\n",
       " '422254 112116 x',\n",
       " '422489 031815 x',\n",
       " '422621 031017 x',\n",
       " '422690 062716 x',\n",
       " '422796 062717 x',\n",
       " '423293 032919 x',\n",
       " '423354_1 032316 x',\n",
       " '423354_2 032618 x',\n",
       " '423378 092914 x',\n",
       " '423384 112216 x',\n",
       " '424266 101419 x',\n",
       " '430348 020215 x',\n",
       " '479303_1 101609 x',\n",
       " '479303_2 101619 x',\n",
       " '514026 021611 x',\n",
       " '524729_1 042310 x',\n",
       " '524729_2 111115 x',\n",
       " '532286_1 091109 x',\n",
       " '544695_1 041519 x',\n",
       " '544695_2 090110 x',\n",
       " '550724 010819 x',\n",
       " '559666 030519 x',\n",
       " '563205 061917 x',\n",
       " '573938_1 082719 x',\n",
       " '573938_2 113012 x',\n",
       " '574000 083118 x',\n",
       " '577477 090418 x',\n",
       " '585387_1 090513 x',\n",
       " '585387_2 091119 x',\n",
       " '600872 041519 x',\n",
       " '602260_1 091707 x',\n",
       " '602260_2 031517 x',\n",
       " '603175 062619 x',\n",
       " '623753 101718 x',\n",
       " '624137 022719 x',\n",
       " '627007 072319 x',\n",
       " '627337 121317 x',\n",
       " '639827 022018 x',\n",
       " '645743 091219 x',\n",
       " '646029_1 061311 x',\n",
       " '646029_2 042516 x',\n",
       " '652530 030818 x',\n",
       " '674179 101218 x',\n",
       " '674726 061518 x',\n",
       " '675597 032718 x',\n",
       " '676633_1 090512 x',\n",
       " '676633_2 082918 x',\n",
       " '677627 082118 x',\n",
       " '678372 041819 x',\n",
       " '679674 102618 x',\n",
       " '679891 022719 x',\n",
       " '681509 061218 x',\n",
       " '681584 061518 x',\n",
       " '681916 083118 x',\n",
       " '682208 062018 x',\n",
       " '682363 091018 x',\n",
       " '683518 012218 x',\n",
       " '684919 082918 x',\n",
       " '684947 090518 x',\n",
       " '684988 92118 x',\n",
       " '685391 100818 x',\n",
       " '685431 092418 x',\n",
       " '691828 011519 x',\n",
       " '692260_1 100312 x',\n",
       " '692260_2 120618 x',\n",
       " '695000_1 071509 x',\n",
       " '695000_2 022819 x',\n",
       " '695120_1 110911 x',\n",
       " '695120_2 022819 x',\n",
       " '698337_1 040313 x',\n",
       " '698337_2 032519 x',\n",
       " '701719 062519 x',\n",
       " '701864 092319 x',\n",
       " '701907 120318 x',\n",
       " '702488 101019 x',\n",
       " '704288 082619 x',\n",
       " '704815 091819 x',\n",
       " '705777 082819 x',\n",
       " '706876 091219 x',\n",
       " '707014 090919 x',\n",
       " '707506 100419 x',\n",
       " '709503 111119 x',\n",
       " '710456 111119 x',\n",
       " 'P090099 010919 x',\n",
       " 'S063947_1 091615 x',\n",
       " 'S063947_2 031419 x',\n",
       " 'S072839 082918 x',\n",
       " 'S092586_1 091117 x',\n",
       " 'S092586_2 011619 x',\n",
       " 'S092813_2 042419 x',\n",
       " 'S095812_1 031214 x',\n",
       " 'S095812_2 031319 x',\n",
       " 'S096180 102519 x',\n",
       " 'S105501_2 091318 x',\n",
       " 'S106364_1 082508 x',\n",
       " 'S106364_2 090215 x',\n",
       " 'S108236_1 070709 x',\n",
       " 'S108236_2 022717 x',\n",
       " 'S111831_1 091113 x',\n",
       " 'S111831_2 041019 x',\n",
       " 'S112423_1 091012 x',\n",
       " 'S112423_2 032116 x',\n",
       " 'S112552_1 102512 x',\n",
       " 'S112552_2 043015 x',\n",
       " 'S113097_1 041309 x',\n",
       " 'S113097_2 042717 x',\n",
       " 'S114954_1 112210 x',\n",
       " 'S114954_2 020619 x',\n",
       " 'S117029_1 060914 x',\n",
       " 'S117029_2 100917 x',\n",
       " 'S421083_1 041813 x',\n",
       " 'S421083_2 052114 x',\n",
       " 'S594966_2 091718 x'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset.ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset[dataset.ID == \"Patrick\"].Path:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_K_Fold(dataframe, augscale, fold_num):\n",
    "        total_stage_3 = len(dataframe[dataframe.State==3])\n",
    "        \n",
    "        def get_ID_frequence(dataframe, augscale):\n",
    "                groups = [ table for patient_ID, table in dataframe.groupby(\"ID\") ]\n",
    "                ID_groups = dataframe.groupby(\"ID\")\n",
    "                frequence = []\n",
    "                total_stage_3 = len(dataframe[dataframe.State==3])\n",
    "                for group_ID, group_table in ID_groups:\n",
    "                        frequence.append([group_ID, len(group_table[group_table.State==3]) // augscale])\n",
    "                return frequence\n",
    "        \n",
    "        frequence = get_ID_frequence(dataframe, augscale)\n",
    "        np.random.shuffle(frequence)\n",
    "        \n",
    "        fraction = round( total_stage_3 / augscale / fold_num )\n",
    "\n",
    "        fold_index = [0]\n",
    "        count = 0\n",
    "        for idx, item in enumerate(frequence):\n",
    "                id_num, freq = item\n",
    "                if count + freq >= fraction:\n",
    "                        count = 0\n",
    "                        fold_index.append(idx)\n",
    "                count += freq\n",
    "        \n",
    "#         fold_index[-1] = len(frequence)\n",
    "        \n",
    "        K_fold_df = []\n",
    "        all_groups = dataframe.groupby(\"ID\")\n",
    "\n",
    "        for i in range(fold_num):\n",
    "                one_partition = np.array(frequence[fold_index[i]:fold_index[i+1]])\n",
    "                one_partition_ids = one_partition[:, 0]\n",
    "                one_partition_groups = [ all_groups.get_group(patient_ID) for patient_ID in one_partition_ids ]\n",
    "                one_partition_dataset = pd.concat(one_partition_groups).reset_index(drop=True)\n",
    "                K_fold_df.append(one_partition_dataset)\n",
    "                \n",
    "        return K_fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dataset(dataframe, augscale, fold_num):\n",
    "        K_fold_df = split_K_Fold(dataframe, augscale, fold_num)\n",
    "        \n",
    "        \n",
    "        train = ['train'] * (fold_num - 2)\n",
    "        order = [ *train, 'valid', 'test']\n",
    "        order = np.array(order)\n",
    "\n",
    "        for rotate_times in range(1, fold_num+1) : \n",
    "                train_dataset, valid_dataset, test_dataset = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "                train_index = np.where(order=='train')[0]\n",
    "                valid_index = np.where(order=='valid')[0][0]\n",
    "                test_index  = np.where(order=='test')[0][0]\n",
    "\n",
    "                for idx in train_index:\n",
    "                        train_dataset   = pd.concat( [train_dataset, K_fold_df[idx] ] ,ignore_index=False )\n",
    "\n",
    "                valid_dataset = K_fold_df[valid_index]\n",
    "                test_dataset  = K_fold_df[test_index]\n",
    "\n",
    "                order = np.roll(order, 1)\n",
    "                \n",
    "                yield train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Fold_print_class_ratio(dataframe):\n",
    "        stage_0 = len(dataframe[dataframe[\"State\"] == 0])\n",
    "        stage_1 = len(dataframe[dataframe[\"State\"] == 1])\n",
    "        stage_2 = len(dataframe[dataframe[\"State\"] == 2])\n",
    "        stage_3 = len(dataframe[dataframe[\"State\"] == 3])\n",
    "        print(\"Class 0 : %d, Class 1 : %d, Class 2 : %d\" % ( (stage_0 + stage_1), stage_2, stage_3 ))\n",
    "        print(\"Stage 0 : %d, Stage 1 : %d, Stage 2 : %d, Stage 3 : %d\" % ( stage_0, stage_1, stage_2, stage_3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Fold_adjust_class_ratio(dataframe, argscale, classes):\n",
    "        new_dataset = pd.DataFrame()\n",
    "        stage_0 = len(dataframe[dataframe[\"State\"] == 0])\n",
    "        stage_1 = len(dataframe[dataframe[\"State\"] == 1])\n",
    "        stage_2 = len(dataframe[dataframe[\"State\"] == 2])\n",
    "        stage_3 = len(dataframe[dataframe[\"State\"] == 3])\n",
    "        \n",
    "        min_num = min(stage_0, stage_1, stage_2, stage_3)\n",
    "        \n",
    "        if classes == 3 : Class_nums = [ min_num // 2, min_num //2, min_num, min_num ]\n",
    "        if classes == 2 : Class_nums = [ min_num , min_num, min_num, min_num ]\n",
    "        \n",
    "        \n",
    "        Stages     = [ 0, 1, 2, 3]\n",
    "        \n",
    "        for Stage, Class_num in zip(Stages, Class_nums):\n",
    "                stage_dataset = dataframe[dataframe[\"State\"] == Stage].reset_index(drop=True)\n",
    "#                 groups = [ stage_dataset.iloc[ i:i+argscale ,:] for i in range(0, len(stage_dataset), argscale) ]\n",
    "                tooth_group = stage_dataset.groupby(\"ID\")\n",
    "                groups = [ table for source, table in tooth_group ]\n",
    "                random.shuffle(groups)\n",
    "                stage_dataset_shuffle = pd.concat(groups).reset_index(drop=True)\n",
    "                get_enough_data_flag, count = False, 0\n",
    "\n",
    "                sample_dict = collections.OrderedDict()\n",
    "                appear_dict = {}\n",
    "                while not get_enough_data_flag:        \n",
    "                        for i in range(0, len(stage_dataset), argscale):\n",
    "                                same_images = stage_dataset_shuffle.iloc[ i:i+argscale ,:].reset_index(drop=True)\n",
    "                                if i not in appear_dict : appear_dict[i] = set()\n",
    "                                \n",
    "                                while True:\n",
    "                                        random_idx = random.randint(0, argscale-1) \n",
    "                                        if random_idx not in appear_dict[i]: break\n",
    "                                \n",
    "                                if len(same_images) != argscale: print(same_images)\n",
    "                                appear_dict[i].add(random_idx)\n",
    "                                append_data = same_images.iloc[random_idx, :]\n",
    "                                sample_dict[count] = append_data.to_dict()\n",
    "                                count += 1\n",
    "                                if count >= Class_num:\n",
    "                                        get_enough_data_flag = True\n",
    "                                        break\n",
    "                stage_sample_dataframe = pd.DataFrame().from_dict(sample_dict).T\n",
    "                new_dataset = pd.concat([new_dataset, stage_sample_dataframe])             \n",
    "        return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Fold_balance_data_generator(dataframe, argscale, classes, batch_size=32, k_fold_num=5):\n",
    "        for train, valid, test in get_all_dataset(dataframe, argscale, k_fold_num):\n",
    "                \n",
    "                print(\"--------------------Before------------------------\")\n",
    "                \n",
    "                K_Fold_print_class_ratio(train)\n",
    "                K_Fold_print_class_ratio(valid)\n",
    "                K_Fold_print_class_ratio(test)\n",
    "                \n",
    "                train_dataset = K_Fold_adjust_class_ratio(train, argscale, classes)\n",
    "                valid_dataset = K_Fold_adjust_class_ratio(valid, argscale, classes)\n",
    "                test_dataset  = K_Fold_adjust_class_ratio(test , argscale, classes)\n",
    "                \n",
    "                print(\"--------------------After------------------------\")\n",
    "                K_Fold_print_class_ratio(train_dataset)\n",
    "                K_Fold_print_class_ratio(valid_dataset)\n",
    "                K_Fold_print_class_ratio(test_dataset)\n",
    "                \n",
    "                print(\"train ID & valid ID\", set(train_dataset.ID) & set(valid_dataset.ID ))\n",
    "                print(\"test ID  & valid ID\", set(test_dataset.ID ) & set(valid_dataset.ID ))\n",
    "                print(\"train ID & test  ID\", set(train_dataset.ID) & set(test_dataset.ID  ))\n",
    "                \n",
    "                print(\"-----------------------------------------------\")\n",
    "                \n",
    "                train_dataset   = shuffle(train_dataset).reset_index(drop=True)\n",
    "                train_generator = make_generator(train_dataset, batch_size)\n",
    "\n",
    "                valid_dataset   = shuffle(valid_dataset).reset_index(drop=True)\n",
    "                valid_generator = make_generator(valid_dataset, batch_size)\n",
    "\n",
    "                test_dataset    = shuffle(test_dataset).reset_index(drop=True)\n",
    "                test_generator  = make_generator(test_dataset, batch_size)\n",
    "                \n",
    "                yield train_dataset, valid_dataset, test_dataset, train_generator, valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Before------------------------\n",
      "Class 0 : 268800, Class 1 : 50080, Class 2 : 24320\n",
      "Stage 0 : 79200, Stage 1 : 189600, Stage 2 : 50080, Stage 3 : 24320\n",
      "Class 0 : 100320, Class 1 : 26400, Class 2 : 7760\n",
      "Stage 0 : 26400, Stage 1 : 73920, Stage 2 : 26400, Stage 3 : 7760\n",
      "Class 0 : 97600, Class 1 : 15280, Class 2 : 7840\n",
      "Stage 0 : 23120, Stage 1 : 74480, Stage 2 : 15280, Stage 3 : 7840\n",
      "--------------------After------------------------\n",
      "Class 0 : 24320, Class 1 : 24320, Class 2 : 24320\n",
      "Stage 0 : 12160, Stage 1 : 12160, Stage 2 : 24320, Stage 3 : 24320\n",
      "Class 0 : 7760, Class 1 : 7760, Class 2 : 7760\n",
      "Stage 0 : 3880, Stage 1 : 3880, Stage 2 : 7760, Stage 3 : 7760\n",
      "Class 0 : 7840, Class 1 : 7840, Class 2 : 7840\n",
      "Stage 0 : 3920, Stage 1 : 3920, Stage 2 : 7840, Stage 3 : 7840\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 340080, Class 1 : 68480, Class 2 : 24000\n",
      "Stage 0 : 94000, Stage 1 : 246080, Stage 2 : 68480, Stage 3 : 24000\n",
      "Class 0 : 97600, Class 1 : 15280, Class 2 : 7840\n",
      "Stage 0 : 23120, Stage 1 : 74480, Stage 2 : 15280, Stage 3 : 7840\n",
      "Class 0 : 29040, Class 1 : 8000, Class 2 : 8080\n",
      "Stage 0 : 11600, Stage 1 : 17440, Stage 2 : 8000, Stage 3 : 8080\n",
      "--------------------After------------------------\n",
      "Class 0 : 24000, Class 1 : 24000, Class 2 : 24000\n",
      "Stage 0 : 12000, Stage 1 : 12000, Stage 2 : 24000, Stage 3 : 24000\n",
      "Class 0 : 7840, Class 1 : 7840, Class 2 : 7840\n",
      "Stage 0 : 3920, Stage 1 : 3920, Stage 2 : 7840, Stage 3 : 7840\n",
      "Class 0 : 8000, Class 1 : 8000, Class 2 : 8000\n",
      "Stage 0 : 4000, Stage 1 : 4000, Stage 2 : 8000, Stage 3 : 8000\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 252560, Class 1 : 50480, Class 2 : 23680\n",
      "Stage 0 : 66080, Stage 1 : 186480, Stage 2 : 50480, Stage 3 : 23680\n",
      "Class 0 : 29040, Class 1 : 8000, Class 2 : 8080\n",
      "Stage 0 : 11600, Stage 1 : 17440, Stage 2 : 8000, Stage 3 : 8080\n",
      "Class 0 : 185120, Class 1 : 33280, Class 2 : 8160\n",
      "Stage 0 : 51040, Stage 1 : 134080, Stage 2 : 33280, Stage 3 : 8160\n",
      "--------------------After------------------------\n",
      "Class 0 : 23680, Class 1 : 23680, Class 2 : 23680\n",
      "Stage 0 : 11840, Stage 1 : 11840, Stage 2 : 23680, Stage 3 : 23680\n",
      "Class 0 : 8000, Class 1 : 8000, Class 2 : 8000\n",
      "Stage 0 : 4000, Stage 1 : 4000, Stage 2 : 8000, Stage 3 : 8000\n",
      "Class 0 : 8160, Class 1 : 8160, Class 2 : 8160\n",
      "Stage 0 : 4080, Stage 1 : 4080, Stage 2 : 8160, Stage 3 : 8160\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 226960, Class 1 : 49680, Class 2 : 23680\n",
      "Stage 0 : 61120, Stage 1 : 165840, Stage 2 : 49680, Stage 3 : 23680\n",
      "Class 0 : 185120, Class 1 : 33280, Class 2 : 8160\n",
      "Stage 0 : 51040, Stage 1 : 134080, Stage 2 : 33280, Stage 3 : 8160\n",
      "Class 0 : 54640, Class 1 : 8800, Class 2 : 8080\n",
      "Stage 0 : 16560, Stage 1 : 38080, Stage 2 : 8800, Stage 3 : 8080\n",
      "--------------------After------------------------\n",
      "Class 0 : 23680, Class 1 : 23680, Class 2 : 23680\n",
      "Stage 0 : 11840, Stage 1 : 11840, Stage 2 : 23680, Stage 3 : 23680\n",
      "Class 0 : 8160, Class 1 : 8160, Class 2 : 8160\n",
      "Stage 0 : 4080, Stage 1 : 4080, Stage 2 : 8160, Stage 3 : 8160\n",
      "Class 0 : 8080, Class 1 : 8080, Class 2 : 8080\n",
      "Stage 0 : 4040, Stage 1 : 4040, Stage 2 : 8080, Stage 3 : 8080\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 311760, Class 1 : 56560, Class 2 : 24080\n",
      "Stage 0 : 85760, Stage 1 : 226000, Stage 2 : 56560, Stage 3 : 24080\n",
      "Class 0 : 54640, Class 1 : 8800, Class 2 : 8080\n",
      "Stage 0 : 16560, Stage 1 : 38080, Stage 2 : 8800, Stage 3 : 8080\n",
      "Class 0 : 100320, Class 1 : 26400, Class 2 : 7760\n",
      "Stage 0 : 26400, Stage 1 : 73920, Stage 2 : 26400, Stage 3 : 7760\n",
      "--------------------After------------------------\n",
      "Class 0 : 24080, Class 1 : 24080, Class 2 : 24080\n",
      "Stage 0 : 12040, Stage 1 : 12040, Stage 2 : 24080, Stage 3 : 24080\n",
      "Class 0 : 8080, Class 1 : 8080, Class 2 : 8080\n",
      "Stage 0 : 4040, Stage 1 : 4040, Stage 2 : 8080, Stage 3 : 8080\n",
      "Class 0 : 7760, Class 1 : 7760, Class 2 : 7760\n",
      "Stage 0 : 3880, Stage 1 : 3880, Stage 2 : 7760, Stage 3 : 7760\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_num = 1\n",
    "class_num = 3\n",
    "for train_dataset, valid_dataset, test_dataset, train_generator, valid_generator, test_generator in K_Fold_balance_data_generator(dataset, argscale=argscale_num, classes=class_num, batch_size=32, k_fold_num=5):\n",
    "        if not os.path.isdir(f\"balance_dataset/Class_{class_num}_3P_multitask/Fold_{fold_num}\"):\n",
    "                os.makedirs(f\"balance_dataset/Class_{class_num}_3P_multitask/Fold_{fold_num}\")\n",
    "        \n",
    "        train_dataset.to_csv(f\"balance_dataset/Class_{class_num}_3P_multitask/Fold_{fold_num}/train_dataset.csv\", index=True)\n",
    "        valid_dataset.to_csv(f\"balance_dataset/Class_{class_num}_3P_multitask/Fold_{fold_num}/valid_dataset.csv\", index=True)\n",
    "        test_dataset.to_csv(f\"balance_dataset/Class_{class_num}_3P_multitask/Fold_{fold_num}/test_dataset.csv\", index=True)\n",
    "        \n",
    "        fold_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
