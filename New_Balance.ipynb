{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from Dental_Tool.Data_processing import *\n",
    "from Dental_Tool.Dental_Model import *\n",
    "from Dental_Tool.Process_results import *\n",
    "from Dental_Tool.Dataloader import *\n",
    "# from Dental_Tool.KFold_v3 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>State</th>\n",
       "      <th>Class</th>\n",
       "      <th>ID</th>\n",
       "      <th>ori_src</th>\n",
       "      <th>source</th>\n",
       "      <th>tooth_num</th>\n",
       "      <th>tooth_type</th>\n",
       "      <th>side</th>\n",
       "      <th>angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_202011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_202011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_L</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_202011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>000408 102419 x</td>\n",
       "      <td>NN_191024_151623_BE78A8</td>\n",
       "      <td>NN_191024_151623_BE78A8_6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NN_191024_151623_BE78A8_6_R</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616555</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_clahe_...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_L</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616556</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_202011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616557</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_202011...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616558</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616559</th>\n",
       "      <td>Dental_Data/PBL/10_interdental_majority_clahe_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S594966_2 091718 x</td>\n",
       "      <td>NN_180917_113933_C0A0B2</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>NN_180917_113933_C0A0B2_26_R</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>616560 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Path  State  Class  \\\n",
       "0       Dental_Data/PBL/10_interdental_majority_202011...      1      0   \n",
       "1       Dental_Data/PBL/10_interdental_majority_202011...      1      0   \n",
       "2       Dental_Data/PBL/10_interdental_majority_clahe_...      1      0   \n",
       "3       Dental_Data/PBL/10_interdental_majority_clahe_...      1      0   \n",
       "4       Dental_Data/PBL/10_interdental_majority_202011...      1      0   \n",
       "...                                                   ...    ...    ...   \n",
       "616555  Dental_Data/PBL/10_interdental_majority_clahe_...      2      1   \n",
       "616556  Dental_Data/PBL/10_interdental_majority_202011...      1      0   \n",
       "616557  Dental_Data/PBL/10_interdental_majority_202011...      1      0   \n",
       "616558  Dental_Data/PBL/10_interdental_majority_clahe_...      1      0   \n",
       "616559  Dental_Data/PBL/10_interdental_majority_clahe_...      1      0   \n",
       "\n",
       "                        ID                  ori_src  \\\n",
       "0          000408 102419 x  NN_191024_151623_BE78A8   \n",
       "1          000408 102419 x  NN_191024_151623_BE78A8   \n",
       "2          000408 102419 x  NN_191024_151623_BE78A8   \n",
       "3          000408 102419 x  NN_191024_151623_BE78A8   \n",
       "4          000408 102419 x  NN_191024_151623_BE78A8   \n",
       "...                    ...                      ...   \n",
       "616555  S594966_2 091718 x  NN_180917_113933_C0A0B2   \n",
       "616556  S594966_2 091718 x  NN_180917_113933_C0A0B2   \n",
       "616557  S594966_2 091718 x  NN_180917_113933_C0A0B2   \n",
       "616558  S594966_2 091718 x  NN_180917_113933_C0A0B2   \n",
       "616559  S594966_2 091718 x  NN_180917_113933_C0A0B2   \n",
       "\n",
       "                            source  tooth_num  tooth_type  \\\n",
       "0        NN_191024_151623_BE78A8_6          6           2   \n",
       "1        NN_191024_151623_BE78A8_6          6           2   \n",
       "2        NN_191024_151623_BE78A8_6          6           2   \n",
       "3        NN_191024_151623_BE78A8_6          6           2   \n",
       "4        NN_191024_151623_BE78A8_6          6           2   \n",
       "...                            ...        ...         ...   \n",
       "616555  NN_180917_113933_C0A0B2_26         26           3   \n",
       "616556  NN_180917_113933_C0A0B2_26         26           3   \n",
       "616557  NN_180917_113933_C0A0B2_26         26           3   \n",
       "616558  NN_180917_113933_C0A0B2_26         26           3   \n",
       "616559  NN_180917_113933_C0A0B2_26         26           3   \n",
       "\n",
       "                                side  angle  \n",
       "0        NN_191024_151623_BE78A8_6_L    -10  \n",
       "1        NN_191024_151623_BE78A8_6_L    -10  \n",
       "2        NN_191024_151623_BE78A8_6_L    -10  \n",
       "3        NN_191024_151623_BE78A8_6_L    -10  \n",
       "4        NN_191024_151623_BE78A8_6_R    -10  \n",
       "...                              ...    ...  \n",
       "616555  NN_180917_113933_C0A0B2_26_L      9  \n",
       "616556  NN_180917_113933_C0A0B2_26_R      9  \n",
       "616557  NN_180917_113933_C0A0B2_26_R      9  \n",
       "616558  NN_180917_113933_C0A0B2_26_R      9  \n",
       "616559  NN_180917_113933_C0A0B2_26_R      9  \n",
       "\n",
       "[616560 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = [ \n",
    "                \"Dental_Data/PBL/10_interdental_majority_20201115_Max_4\", \n",
    "                \"Dental_Data/PBL/10_interdental_majority_20201115_Max_4_Flip\", \n",
    "                \"Dental_Data/PBL/10_interdental_majority_clahe_20201115_Max_4\", \n",
    "                \"Dental_Data/PBL/10_interdental_majority_clahe_20201115_Max_4_Flip\",\n",
    "            ]\n",
    "\n",
    "directory = [ i + \"/mapping.json\" for i in directory]\n",
    "argscale_num = len(directory) * 20 \n",
    "\n",
    "data = load_json(directory, interdental=True)\n",
    "dataset = json_2_dataframe_PBL_inderdental(data)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000408 102419 x',\n",
       " '000411 112119 x',\n",
       " '001742 082712 x',\n",
       " '002456 060517 x',\n",
       " '002555 042513 x',\n",
       " '003262 103015 x',\n",
       " '003615 010816 x',\n",
       " '003670 020718 x',\n",
       " '004151 091409 x',\n",
       " '004359 030716 x',\n",
       " '004499 110515 x',\n",
       " '005627 120209 x',\n",
       " '007274 021016 x',\n",
       " '007501 082216 x',\n",
       " '008908 090309 x',\n",
       " '010801 011019 x',\n",
       " '010953 031618 x',\n",
       " '019747_1 033011 x',\n",
       " '019747_2 061417 x',\n",
       " '022198_1 080717 x',\n",
       " '022198_2 100919 x',\n",
       " '025179_1 082510 x',\n",
       " '025179_2 101216 x',\n",
       " '026110_1 030513 x',\n",
       " '026110_2 061819 x',\n",
       " '026118_1 110613 x',\n",
       " '026118_2 060718 x',\n",
       " '035015_1 022613 x',\n",
       " '035015_2 082619 x',\n",
       " '043521_1 082113 x',\n",
       " '043521_2 082318 x',\n",
       " '051282_1 032311 x',\n",
       " '051282_2 102115 x',\n",
       " '051282_3 102918 x',\n",
       " '060011_1 033010 x',\n",
       " '060011_2 032817 x',\n",
       " '064768_1 022912 x',\n",
       " '064768_2 062513 x',\n",
       " '068783_1 090309 x',\n",
       " '074670_1 020316 x',\n",
       " '074670_2 071718 x',\n",
       " '077345_1 030719 x',\n",
       " '077345_2 082312 x',\n",
       " '1028067 062718 x',\n",
       " '10689 102418 x',\n",
       " '111084_1 071918 x',\n",
       " '111084_2 021017 x',\n",
       " '13529 092513 x',\n",
       " '1899 120718 x',\n",
       " '415134_1 111210 x',\n",
       " '415134_2 111218 x',\n",
       " '417511 080918 x',\n",
       " '417607 021716 x',\n",
       " '419024_1 110712 x',\n",
       " '419024_2 101817 x',\n",
       " '419477 091415 x',\n",
       " '419790_1 021216 x',\n",
       " '419790_2 061219 x',\n",
       " '420606_1 032409 x',\n",
       " '420888 040416 x',\n",
       " '421067 041615 x',\n",
       " '421661 092517 x',\n",
       " '421730 060716 x',\n",
       " '422241 121212 x',\n",
       " '422254 112116 x',\n",
       " '422489 031815 x',\n",
       " '422621 031017 x',\n",
       " '422690 062716 x',\n",
       " '422796 062717 x',\n",
       " '423293 032919 x',\n",
       " '423354_1 032316 x',\n",
       " '423354_2 032618 x',\n",
       " '423378 092914 x',\n",
       " '423384 112216 x',\n",
       " '424266 101419 x',\n",
       " '430348 020215 x',\n",
       " '479303_1 101609 x',\n",
       " '479303_2 101619 x',\n",
       " '514026 021611 x',\n",
       " '524729_1 042310 x',\n",
       " '524729_2 111115 x',\n",
       " '527495 071019 x',\n",
       " '532286_1 091109 x',\n",
       " '532286_2 111418 x',\n",
       " '544695_1 041519 x',\n",
       " '544695_2 090110 x',\n",
       " '550724 010819 x',\n",
       " '559666 030519 x',\n",
       " '563205 061917 x',\n",
       " '573938_1 082719 x',\n",
       " '573938_2 113012 x',\n",
       " '574000 083118 x',\n",
       " '577477 090418 x',\n",
       " '585387_1 090513 x',\n",
       " '585387_2 091119 x',\n",
       " '592428_1 013014 x',\n",
       " '592428_2 102518 x',\n",
       " '600872 041519 x',\n",
       " '602260_1 091707 x',\n",
       " '602260_2 031517 x',\n",
       " '603175 062619 x',\n",
       " '623753 101718 x',\n",
       " '624137 022719 x',\n",
       " '627007 072319 x',\n",
       " '627337 121317 x',\n",
       " '639827 022018 x',\n",
       " '645743 091219 x',\n",
       " '646029_1 061311 x',\n",
       " '646029_2 042516 x',\n",
       " '652530 030818 x',\n",
       " '674179 101218 x',\n",
       " '674726 061518 x',\n",
       " '675597 032718 x',\n",
       " '676633_1 090512 x',\n",
       " '676633_2 082918 x',\n",
       " '677627 082118 x',\n",
       " '678372 041819 x',\n",
       " '679674 102618 x',\n",
       " '679891 022719 x',\n",
       " '681509 061218 x',\n",
       " '681584 061518 x',\n",
       " '681916 083118 x',\n",
       " '682208 062018 x',\n",
       " '682363 091018 x',\n",
       " '683518 012218 x',\n",
       " '684919 082918 x',\n",
       " '684947 090518 x',\n",
       " '684988 92118 x',\n",
       " '685391 100818 x',\n",
       " '685431 092418 x',\n",
       " '691828 011519 x',\n",
       " '692260_1 100312 x',\n",
       " '692260_2 120618 x',\n",
       " '695000_1 071509 x',\n",
       " '695000_2 022819 x',\n",
       " '695120_1 110911 x',\n",
       " '695120_2 022819 x',\n",
       " '698337_1 040313 x',\n",
       " '698337_2 032519 x',\n",
       " '701509 061919 x',\n",
       " '701719 062519 x',\n",
       " '701864 092319 x',\n",
       " '701907 120318 x',\n",
       " '702488 101019 x',\n",
       " '704288 082619 x',\n",
       " '704815 091819 x',\n",
       " '705777 082819 x',\n",
       " '706876 091219 x',\n",
       " '707014 090919 x',\n",
       " '707506 100419 x',\n",
       " '709503 111119 x',\n",
       " '710456 111119 x',\n",
       " 'P090099 010919 x',\n",
       " 'S024961_1 032614 x',\n",
       " 'S063947_1 091615 x',\n",
       " 'S063947_2 031419 x',\n",
       " 'S072839 082918 x',\n",
       " 'S092586_1 091117 x',\n",
       " 'S092586_2 011619 x',\n",
       " 'S092813_1 103013 x',\n",
       " 'S092813_2 042419 x',\n",
       " 'S095812_1 031214 x',\n",
       " 'S095812_2 031319 x',\n",
       " 'S096180 102519 x',\n",
       " 'S105501_2 091318 x',\n",
       " 'S106364_1 082508 x',\n",
       " 'S106364_2 090215 x',\n",
       " 'S108236_1 070709 x',\n",
       " 'S108236_2 022717 x',\n",
       " 'S111831_1 091113 x',\n",
       " 'S111831_2 041019 x',\n",
       " 'S112423_1 091012 x',\n",
       " 'S112423_2 032116 x',\n",
       " 'S112552_1 102512 x',\n",
       " 'S112552_2 043015 x',\n",
       " 'S113097_1 041309 x',\n",
       " 'S113097_2 042717 x',\n",
       " 'S114954_1 112210 x',\n",
       " 'S114954_2 020619 x',\n",
       " 'S117029_1 060914 x',\n",
       " 'S117029_2 100917 x',\n",
       " 'S421083_1 041813 x',\n",
       " 'S421083_2 052114 x',\n",
       " 'S594966_2 091718 x'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset.ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheng20 = [   \"002555 042513 x\", \"000408 102419 x\" , \"005627 120209 x\", \"004151 091409 x\",\n",
    "#     \"003615 010816 x\", \"007274 021016 x\" , \"007274 021016 x\", \"004359 030716 x\",\n",
    "#     \"001742 082712 x\", \"002456 060517 x\" , \"1899 120718 x\"  , \"004499 110515 x\",\n",
    "#     \"13529 092513 x\" ,  \"000411 112119 x\", \"008908 090309 x\", \"003262 103015 x\",\n",
    "#     \"10689 102418 x\" , \"003670 020718 x\" , \"010953 031618 x\", ]   \n",
    "\n",
    "\n",
    "# dataset = dataset[~dataset.ID.isin(sheng20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_K_Fold(dataframe, augscale, fold_num):\n",
    "        total_stage_3 = len(dataframe[dataframe.State==3])\n",
    "        \n",
    "        def get_ID_frequence(dataframe, augscale):\n",
    "                groups = [ table for patient_ID, table in dataframe.groupby(\"ID\") ]\n",
    "                ID_groups = dataframe.groupby(\"ID\")\n",
    "                frequence = []\n",
    "                total_stage_3 = len(dataframe[dataframe.State==3])\n",
    "                for group_ID, group_table in ID_groups:\n",
    "                        frequence.append([group_ID, len(group_table[group_table.State==3]) // augscale])\n",
    "                return frequence\n",
    "        \n",
    "        frequence = get_ID_frequence(dataframe, augscale)\n",
    "        np.random.shuffle(frequence)\n",
    "        \n",
    "        fraction = round( total_stage_3 / augscale / fold_num )\n",
    "\n",
    "        fold_index = [0]\n",
    "        count = 0\n",
    "        for idx, item in enumerate(frequence):\n",
    "                id_num, freq = item\n",
    "                if count + freq >= fraction:\n",
    "                        count = 0\n",
    "                        fold_index.append(idx)\n",
    "                count += freq\n",
    "        \n",
    "#         fold_index[-1] = len(frequence)\n",
    "        \n",
    "        K_fold_df = []\n",
    "        all_groups = dataframe.groupby(\"ID\")\n",
    "\n",
    "        for i in range(fold_num):\n",
    "                one_partition = np.array(frequence[fold_index[i]:fold_index[i+1]])\n",
    "                one_partition_ids = one_partition[:, 0]\n",
    "                one_partition_groups = [ all_groups.get_group(patient_ID) for patient_ID in one_partition_ids ]\n",
    "                one_partition_dataset = pd.concat(one_partition_groups).reset_index(drop=True)\n",
    "                K_fold_df.append(one_partition_dataset)\n",
    "                \n",
    "        return K_fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dataset(dataframe, augscale, fold_num):\n",
    "        K_fold_df = split_K_Fold(dataframe, augscale, fold_num)\n",
    "        \n",
    "        \n",
    "        train = ['train'] * (fold_num - 2)\n",
    "        order = [ *train, 'valid', 'test']\n",
    "        order = np.array(order)\n",
    "\n",
    "        for rotate_times in range(1, fold_num+1) : \n",
    "                train_dataset, valid_dataset, test_dataset = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "                train_index = np.where(order=='train')[0]\n",
    "                valid_index = np.where(order=='valid')[0][0]\n",
    "                test_index  = np.where(order=='test')[0][0]\n",
    "\n",
    "                for idx in train_index:\n",
    "                        train_dataset   = pd.concat( [train_dataset, K_fold_df[idx] ] ,ignore_index=False )\n",
    "\n",
    "                valid_dataset = K_fold_df[valid_index]\n",
    "                test_dataset  = K_fold_df[test_index]\n",
    "\n",
    "                order = np.roll(order, 1)\n",
    "                \n",
    "                yield train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Fold_print_class_ratio(dataframe):\n",
    "        stage_0 = len(dataframe[dataframe[\"State\"] == 0])\n",
    "        stage_1 = len(dataframe[dataframe[\"State\"] == 1])\n",
    "        stage_2 = len(dataframe[dataframe[\"State\"] == 2])\n",
    "        stage_3 = len(dataframe[dataframe[\"State\"] == 3])\n",
    "        print(\"Class 0 : %d, Class 1 : %d, Class 2 : %d\" % ( (stage_0 + stage_1), stage_2, stage_3 ))\n",
    "        print(\"Stage 0 : %d, Stage 1 : %d, Stage 2 : %d, Stage 3 : %d\" % ( stage_0, stage_1, stage_2, stage_3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Fold_adjust_class_ratio(dataframe, argscale, classes):\n",
    "        new_dataset = pd.DataFrame()\n",
    "        stage_0 = len(dataframe[dataframe[\"State\"] == 0])\n",
    "        stage_1 = len(dataframe[dataframe[\"State\"] == 1])\n",
    "        stage_2 = len(dataframe[dataframe[\"State\"] == 2])\n",
    "        stage_3 = len(dataframe[dataframe[\"State\"] == 3])\n",
    "        \n",
    "        min_num = min(stage_0, stage_1, stage_2, stage_3)\n",
    "        \n",
    "        if classes == 3 : Class_nums = [ min_num // 2, min_num //2, min_num, min_num ]\n",
    "        if classes == 2 : Class_nums = [ min_num , min_num, min_num, min_num ]\n",
    "        \n",
    "        \n",
    "        Stages     = [ 0, 1, 2, 3]\n",
    "        \n",
    "        for Stage, Class_num in zip(Stages, Class_nums):\n",
    "                stage_dataset = dataframe[dataframe[\"State\"] == Stage].reset_index(drop=True)\n",
    "#                 groups = [ stage_dataset.iloc[ i:i+argscale ,:] for i in range(0, len(stage_dataset), argscale) ]\n",
    "                tooth_group = stage_dataset.groupby(\"ID\")\n",
    "                groups = [ table for source, table in tooth_group ]\n",
    "                random.shuffle(groups)\n",
    "                stage_dataset_shuffle = pd.concat(groups).reset_index(drop=True)\n",
    "                get_enough_data_flag, count = False, 0\n",
    "\n",
    "                sample_dict = collections.OrderedDict()\n",
    "                appear_dict = {}\n",
    "                while not get_enough_data_flag:        \n",
    "                        for i in range(0, len(stage_dataset), argscale):\n",
    "                                same_images = stage_dataset_shuffle.iloc[ i:i+argscale ,:].reset_index(drop=True)\n",
    "                                if i not in appear_dict : appear_dict[i] = set()\n",
    "                                \n",
    "                                while True:\n",
    "                                        random_idx = random.randint(0, argscale-1) \n",
    "                                        if random_idx not in appear_dict[i]: break\n",
    "                                \n",
    "                                if len(same_images) != argscale: print(same_images)\n",
    "                                appear_dict[i].add(random_idx)\n",
    "                                append_data = same_images.iloc[random_idx, :]\n",
    "                                sample_dict[count] = append_data.to_dict()\n",
    "                                count += 1\n",
    "                                if count >= Class_num:\n",
    "                                        get_enough_data_flag = True\n",
    "                                        break\n",
    "                stage_sample_dataframe = pd.DataFrame().from_dict(sample_dict).T\n",
    "                new_dataset = pd.concat([new_dataset, stage_sample_dataframe])             \n",
    "        return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Fold_balance_data_generator(dataframe, argscale, classes, batch_size=32, k_fold_num=5):\n",
    "        for train, valid, test in get_all_dataset(dataframe, argscale, k_fold_num):\n",
    "                \n",
    "                print(\"--------------------Before------------------------\")\n",
    "                \n",
    "                K_Fold_print_class_ratio(train)\n",
    "                K_Fold_print_class_ratio(valid)\n",
    "                K_Fold_print_class_ratio(test)\n",
    "                \n",
    "                train_dataset = K_Fold_adjust_class_ratio(train, argscale, classes)\n",
    "                valid_dataset = K_Fold_adjust_class_ratio(valid, argscale, classes)\n",
    "                test_dataset  = K_Fold_adjust_class_ratio(test , argscale, classes)\n",
    "                \n",
    "                print(\"--------------------After------------------------\")\n",
    "                K_Fold_print_class_ratio(train_dataset)\n",
    "                K_Fold_print_class_ratio(valid_dataset)\n",
    "                K_Fold_print_class_ratio(test_dataset)\n",
    "                \n",
    "                print(\"train ID & valid ID\", set(train_dataset.ID) & set(valid_dataset.ID ))\n",
    "                print(\"test ID  & valid ID\", set(test_dataset.ID ) & set(valid_dataset.ID ))\n",
    "                print(\"train ID & test  ID\", set(train_dataset.ID) & set(test_dataset.ID  ))\n",
    "                \n",
    "                print(\"-----------------------------------------------\")\n",
    "                \n",
    "                train_dataset   = shuffle(train_dataset).reset_index(drop=True)\n",
    "                train_generator = make_generator(train_dataset, batch_size)\n",
    "\n",
    "                valid_dataset   = shuffle(valid_dataset).reset_index(drop=True)\n",
    "                valid_generator = make_generator(valid_dataset, batch_size)\n",
    "\n",
    "                test_dataset    = shuffle(test_dataset).reset_index(drop=True)\n",
    "                test_generator  = make_generator(test_dataset, batch_size)\n",
    "                \n",
    "                yield train_dataset, valid_dataset, test_dataset, train_generator, valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Before------------------------\n",
      "Class 0 : 254720, Class 1 : 52080, Class 2 : 25280\n",
      "Stage 0 : 76800, Stage 1 : 177920, Stage 2 : 52080, Stage 3 : 25280\n",
      "Class 0 : 91760, Class 1 : 29280, Class 2 : 8320\n",
      "Stage 0 : 28160, Stage 1 : 63600, Stage 2 : 29280, Stage 3 : 8320\n",
      "Class 0 : 97680, Class 1 : 26320, Class 2 : 8320\n",
      "Stage 0 : 28400, Stage 1 : 69280, Stage 2 : 26320, Stage 3 : 8320\n",
      "--------------------After------------------------\n",
      "Class 0 : 25280, Class 1 : 25280, Class 2 : 25280\n",
      "Stage 0 : 12640, Stage 1 : 12640, Stage 2 : 25280, Stage 3 : 25280\n",
      "Class 0 : 8320, Class 1 : 8320, Class 2 : 8320\n",
      "Stage 0 : 4160, Stage 1 : 4160, Stage 2 : 8320, Stage 3 : 8320\n",
      "Class 0 : 8320, Class 1 : 8320, Class 2 : 8320\n",
      "Stage 0 : 4160, Stage 1 : 4160, Stage 2 : 8320, Stage 3 : 8320\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 286080, Class 1 : 70640, Class 2 : 25200\n",
      "Stage 0 : 93680, Stage 1 : 192400, Stage 2 : 70640, Stage 3 : 25200\n",
      "Class 0 : 97680, Class 1 : 26320, Class 2 : 8320\n",
      "Stage 0 : 28400, Stage 1 : 69280, Stage 2 : 26320, Stage 3 : 8320\n",
      "Class 0 : 60400, Class 1 : 10720, Class 2 : 8400\n",
      "Stage 0 : 11280, Stage 1 : 49120, Stage 2 : 10720, Stage 3 : 8400\n",
      "--------------------After------------------------\n",
      "Class 0 : 25200, Class 1 : 25200, Class 2 : 25200\n",
      "Stage 0 : 12600, Stage 1 : 12600, Stage 2 : 25200, Stage 3 : 25200\n",
      "Class 0 : 8320, Class 1 : 8320, Class 2 : 8320\n",
      "Stage 0 : 4160, Stage 1 : 4160, Stage 2 : 8320, Stage 3 : 8320\n",
      "Class 0 : 8400, Class 1 : 8400, Class 2 : 8400\n",
      "Stage 0 : 4200, Stage 1 : 4200, Stage 2 : 8400, Stage 3 : 8400\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 293760, Class 1 : 80400, Class 2 : 25120\n",
      "Stage 0 : 88480, Stage 1 : 205280, Stage 2 : 80400, Stage 3 : 25120\n",
      "Class 0 : 60400, Class 1 : 10720, Class 2 : 8400\n",
      "Stage 0 : 11280, Stage 1 : 49120, Stage 2 : 10720, Stage 3 : 8400\n",
      "Class 0 : 90000, Class 1 : 16560, Class 2 : 8400\n",
      "Stage 0 : 33600, Stage 1 : 56400, Stage 2 : 16560, Stage 3 : 8400\n",
      "--------------------After------------------------\n",
      "Class 0 : 25120, Class 1 : 25120, Class 2 : 25120\n",
      "Stage 0 : 12560, Stage 1 : 12560, Stage 2 : 25120, Stage 3 : 25120\n",
      "Class 0 : 8400, Class 1 : 8400, Class 2 : 8400\n",
      "Stage 0 : 4200, Stage 1 : 4200, Stage 2 : 8400, Stage 3 : 8400\n",
      "Class 0 : 8400, Class 1 : 8400, Class 2 : 8400\n",
      "Stage 0 : 4200, Stage 1 : 4200, Stage 2 : 8400, Stage 3 : 8400\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 249840, Class 1 : 66320, Class 2 : 25040\n",
      "Stage 0 : 67840, Stage 1 : 182000, Stage 2 : 66320, Stage 3 : 25040\n",
      "Class 0 : 90000, Class 1 : 16560, Class 2 : 8400\n",
      "Stage 0 : 33600, Stage 1 : 56400, Stage 2 : 16560, Stage 3 : 8400\n",
      "Class 0 : 104320, Class 1 : 24800, Class 2 : 8480\n",
      "Stage 0 : 31920, Stage 1 : 72400, Stage 2 : 24800, Stage 3 : 8480\n",
      "--------------------After------------------------\n",
      "Class 0 : 25040, Class 1 : 25040, Class 2 : 25040\n",
      "Stage 0 : 12520, Stage 1 : 12520, Stage 2 : 25040, Stage 3 : 25040\n",
      "Class 0 : 8400, Class 1 : 8400, Class 2 : 8400\n",
      "Stage 0 : 4200, Stage 1 : 4200, Stage 2 : 8400, Stage 3 : 8400\n",
      "Class 0 : 8480, Class 1 : 8480, Class 2 : 8480\n",
      "Stage 0 : 4240, Stage 1 : 4240, Stage 2 : 8480, Stage 3 : 8480\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n",
      "--------------------Before------------------------\n",
      "Class 0 : 248080, Class 1 : 53600, Class 2 : 25120\n",
      "Stage 0 : 73280, Stage 1 : 174800, Stage 2 : 53600, Stage 3 : 25120\n",
      "Class 0 : 104320, Class 1 : 24800, Class 2 : 8480\n",
      "Stage 0 : 31920, Stage 1 : 72400, Stage 2 : 24800, Stage 3 : 8480\n",
      "Class 0 : 91760, Class 1 : 29280, Class 2 : 8320\n",
      "Stage 0 : 28160, Stage 1 : 63600, Stage 2 : 29280, Stage 3 : 8320\n",
      "--------------------After------------------------\n",
      "Class 0 : 25120, Class 1 : 25120, Class 2 : 25120\n",
      "Stage 0 : 12560, Stage 1 : 12560, Stage 2 : 25120, Stage 3 : 25120\n",
      "Class 0 : 8480, Class 1 : 8480, Class 2 : 8480\n",
      "Stage 0 : 4240, Stage 1 : 4240, Stage 2 : 8480, Stage 3 : 8480\n",
      "Class 0 : 8320, Class 1 : 8320, Class 2 : 8320\n",
      "Stage 0 : 4160, Stage 1 : 4160, Stage 2 : 8320, Stage 3 : 8320\n",
      "train ID & valid ID set()\n",
      "test ID  & valid ID set()\n",
      "train ID & test  ID set()\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_num = 1\n",
    "class_num = 3\n",
    "\n",
    "dir_name = f\"Class_{class_num}_majority_70_Chang_168\"\n",
    "\n",
    "for train_dataset, valid_dataset, test_dataset, train_generator, valid_generator, test_generator in K_Fold_balance_data_generator(dataset, argscale=argscale_num, classes=class_num, batch_size=32, k_fold_num=5):\n",
    "        if not os.path.isdir(f\"balance_dataset/{dir_name}/Fold_{fold_num}\"):\n",
    "                os.makedirs(f\"balance_dataset/{dir_name}/Fold_{fold_num}\")\n",
    "        \n",
    "        train_dataset.to_csv(f\"balance_dataset/{dir_name}/Fold_{fold_num}/train_dataset.csv\", index=True)\n",
    "        valid_dataset.to_csv(f\"balance_dataset/{dir_name}/Fold_{fold_num}/valid_dataset.csv\", index=True)\n",
    "        test_dataset.to_csv(f\"balance_dataset/{dir_name}/Fold_{fold_num}/test_dataset.csv\", index=True)\n",
    "        \n",
    "        fold_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
